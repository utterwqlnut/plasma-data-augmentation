2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Current SDK version is 0.16.4
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Configure stats pid to 8855
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Loading settings from /Users/dhruvachayapathy/.config/wandb/settings
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Loading settings from /Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/wandb/settings
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'entity': 'timetodisrupt', 'project': 'testing dhruva_ttd', 'sweep_id': 'u4hj6lvx', 'root_dir': '/Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src', 'run_id': 'nc7087dy', 'sweep_param_path': '/Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/wandb/sweep-u4hj6lvx/config-nc7087dy.yaml'}
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-03-29 03:56:51,460 INFO    MainThread:8855 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'src/main.py', 'program_abspath': '/Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/main.py', 'program': '/Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/main.py'}
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:_log_setup():526] Logging user logs to /Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/wandb/run-20240329_035651-nc7087dy/logs/debug.log
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:_log_setup():527] Logging internal logs to /Users/dhruvachayapathy/personal/research/mit-research/plasma-data-augmentation/src/wandb/run-20240329_035651-nc7087dy/logs/debug-internal.log
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:init():566] calling init triggers
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:init():573] wandb.init called with sweep_config: {'balance': False, 'e_lr': 0.0001, 'encoder_hidden_dim': 32, 'encoder_n_layers': 3, 'post_hoc_h_size': 32, 'post_hoc_lr': 1e-05, 'post_hoc_n_layers': 4, 'v_lr': 0.0001, 'viewmaker_activation': 'relu', 'viewmaker_distortion_budget': 0.31513617850959874, 'viewmaker_hidden_dim': 64, 'viewmaker_layer_type': 'lstm', 'viewmaker_loss_t': 0.03907149101309772, 'viewmaker_loss_weight': 0.35127367549386884, 'viewmaker_n_head': 2, 'viewmaker_n_layers': 2}
config: {}
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:init():616] starting backend
2024-03-29 03:56:51,461 INFO    MainThread:8855 [wandb_init.py:init():620] setting up manager
2024-03-29 03:56:51,472 INFO    MainThread:8855 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2024-03-29 03:56:51,475 INFO    MainThread:8855 [wandb_init.py:init():628] backend started and connected
2024-03-29 03:56:51,488 INFO    MainThread:8855 [wandb_run.py:_config_callback():1343] config_cb None None {'balance': False, 'e_lr': 0.0001, 'encoder_hidden_dim': 32, 'encoder_n_layers': 3, 'post_hoc_h_size': 32, 'post_hoc_lr': 1e-05, 'post_hoc_n_layers': 4, 'v_lr': 0.0001, 'viewmaker_activation': 'relu', 'viewmaker_distortion_budget': 0.31513617850959874, 'viewmaker_hidden_dim': 64, 'viewmaker_layer_type': 'lstm', 'viewmaker_loss_t': 0.03907149101309772, 'viewmaker_loss_weight': 0.35127367549386884, 'viewmaker_n_head': 2, 'viewmaker_n_layers': 2}
2024-03-29 03:56:51,490 INFO    MainThread:8855 [wandb_init.py:init():720] updated telemetry
2024-03-29 03:56:51,531 INFO    MainThread:8855 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2024-03-29 03:56:51,850 INFO    MainThread:8855 [wandb_run.py:_on_init():2262] communicating current version
2024-03-29 03:56:52,039 INFO    MainThread:8855 [wandb_run.py:_on_init():2271] got version response upgrade_message: "wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-03-29 03:56:52,039 INFO    MainThread:8855 [wandb_init.py:init():804] starting run threads in backend
2024-03-29 03:56:55,499 INFO    MainThread:8855 [wandb_run.py:_console_start():2241] atexit reg
2024-03-29 03:56:55,500 INFO    MainThread:8855 [wandb_run.py:_redirect():2096] redirect: wrap_raw
2024-03-29 03:56:55,500 INFO    MainThread:8855 [wandb_run.py:_redirect():2161] Wrapping output streams.
2024-03-29 03:56:55,500 INFO    MainThread:8855 [wandb_run.py:_redirect():2186] Redirects installed.
2024-03-29 03:56:55,501 INFO    MainThread:8855 [wandb_init.py:init():847] run started, returning control to user process
2024-03-29 03:56:55,540 INFO    MainThread:8855 [wandb_run.py:_config_callback():1343] config_cb None None {'batch_size': 12, 't': 0.03907149101309772, 'v_loss_weight': 0.35127367549386884, 'viewmaker': 'TimeSeriesViewMaker(\n  (activation): ReLU()\n  (net): Sequential(\n    (0): LSTM(13, 64, num_layers=2, batch_first=True)\n    (1): extract_tensor()\n    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n    (5): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n  )\n)', 'encoder': 'PlasmaViewEncoderLSTM(\n  (lstm): LSTM(12, 32, num_layers=3, batch_first=True)\n  (out): Sequential(\n    (0): Linear(in_features=32, out_features=12, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=12, out_features=12, bias=True)\n  )\n)', 'train_dataset': '<data.PlasmaDataset object at 0x7f8dd6ce7f40>', 'val_dataset': '<data.PlasmaDataset object at 0x7f8de5781ae0>', 'collate_fn': 'data.viewmaker_collate_fn', 'included_machines': ['cmod', 'east', 'd3d'], 'viewmaker_batch_size': 12, 'viewmaker_num_epochs': 10, 'post_hoc_num_epochs': 10, 'post_hoc_save_metric': 'accuracy', 'post_hoc_batch_size': 12}
2024-03-29 07:35:16,983 WARNING MsgRouterThr:8855 [router.py:message_loop():77] message_loop has been closed
